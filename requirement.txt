numpy==2.2.6
pandas==2.3.3
torch==2.8.0
torchvision==0.23.0
torchaudio==2.8.0
xformers==0.0.32.post1
timm==1.0.16
accelerate==1.10.1
einops==0.8.1
sentencepiece==0.2.1
safetensors==0.6.2
tokenizers==0.22.1
tiktoken==0.12.0
transformers==4.57.0
peft==0.17.1
fastapi==0.119.0
uvicorn==0.37.0
typer==0.15.2
ray[default]==2.49.2
openai>=1.99.1
hf_transfer==0.1.9
huggingface-hub==0.35.3
hydra-core==1.3.2
python-dotenv==1.2.1
PyYAML==6.0.3
pydantic==2.12.2
orjson==3.11.3
rich==13.9.4
loguru==0.7.3
typeguard==4.3.0
wandb==0.22.0
decord==0.6.0
av==15.1.0
opencv-python-headless==4.12.0.88
librosa==0.10.1
soundfile==0.13.1
nvidia-ml-py==12.560.30
vllm==0.11.0
tensordict
torchdata
codetiming
datasets
qwen-vl-utils
https://github.com/Dao-AILab/flash-attention/releases/download/v2.8.3/flash_attn-2.8.3+cu12torch2.8cxx11abiFALSE-cp311-cp311-linux_x86_64.whl