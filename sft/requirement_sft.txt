--extra-index-url https://download.pytorch.org/whl/cu124

torch==2.6.0
torchvision==0.21.0
torchaudio==2.6.0

transformers==4.57.1
deepspeed==0.17.1
triton==3.2.0
accelerate==1.7.0
torchcodec==0.2
peft==0.17.1
importlib_metadata
wandb

https://github.com/Dao-AILab/flash-attention/releases/download/v2.7.4.post1/flash_attn-2.7.4.post1+cu12torch2.6cxx11abiFALSE-cp39-cp39-linux_x86_64.whl